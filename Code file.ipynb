{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLBA_Kaggle2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkZxKQ2Z2Zs1"
      },
      "source": [
        "#Importing all the required libraries\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH4sP0o3mZPa"
      },
      "source": [
        "# # # path for train and test data\n",
        "# train_data = '//content//drive//My Drive//MLBA_Kaggle_2//kaggle_train.csv'\n",
        "# test_data = '//content//drive//My Drive//MLBA_Kaggle_2//kaggle_test.csv' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CULWw-PRPlOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a955ad7-cf7a-467f-e10e-1ca7a61d338d"
      },
      "source": [
        "# # Input taken from the user\n",
        "\n",
        "train_data = input('Enter the path name of the train data  :  ')\n",
        "test_data = input('Enter the path name of the training data  :  ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the path name of the train data  :  //content//drive//My Drive//MLBA_Kaggle_2//kaggle_train.csv\n",
            "Enter the path name of the training data  :  //content//drive//My Drive//MLBA_Kaggle_2//kaggle_test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieX1tynrmZWK"
      },
      "source": [
        "#Converting the data into pandas dataframes\n",
        "training_data = pd.read_csv(train_data) \n",
        "testing_data = pd.read_csv(test_data) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1JqgyYGmZgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ddfa758-c9f6-4444-ea0a-98ae444155bc"
      },
      "source": [
        "# Prints shape of training data\n",
        "print(\"Shape of the traning data is \",training_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the traning data is  (242, 320)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdRR8Q3lmZmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57072fe6-8b92-4606-e878-3afe5983f0f7"
      },
      "source": [
        "# Prints shape of training data\n",
        "print(\"Shape of the testing data is \",testing_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the testing data is  (62, 319)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8-VrV-UmZqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffbe3667-8ac1-444f-e640-8e81041112f7"
      },
      "source": [
        "#Print training data \n",
        "print(\"Fews rows of training data\")\n",
        "print(training_data.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fews rows of training data\n",
            "             ID  Labels     ACAN  ...      WDFY1       XIAP    ZCCHC3\n",
            "0  TCGA-JW-A5VG       0  13.6584  ...   923.0769  1046.0021  819.1217\n",
            "1  TCGA-HG-A2PA       0  22.0844  ...  1473.7825  1618.8684  615.8465\n",
            "2  TCGA-C5-A0TN       0  32.7422  ...   477.2417  1064.6161  804.1672\n",
            "3  TCGA-C5-A1BM       0  22.7975  ...  1596.5997  1666.1515  402.6275\n",
            "4  TCGA-EK-A2RA       0   2.5000  ...  1651.5625  1250.3125  982.5000\n",
            "\n",
            "[5 rows x 320 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EogwFqNl9yQq"
      },
      "source": [
        "#Here we can delete the ID column as it is not used in the prediction\n",
        "\n",
        "training_data = training_data.drop(['ID'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "CblPsVO3-Bzt",
        "outputId": "5e0b6abe-22ca-478e-af57-20a8c420aa57"
      },
      "source": [
        "# Training data after removing the ID column\n",
        "\n",
        "training_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labels</th>\n",
              "      <th>ACAN</th>\n",
              "      <th>AGER</th>\n",
              "      <th>ALPK1</th>\n",
              "      <th>ANKRD17</th>\n",
              "      <th>APOB</th>\n",
              "      <th>APPL1</th>\n",
              "      <th>APPL2</th>\n",
              "      <th>ARRB2</th>\n",
              "      <th>ASGR1</th>\n",
              "      <th>ASGR2</th>\n",
              "      <th>ATRN</th>\n",
              "      <th>ATRNL1</th>\n",
              "      <th>BCAN</th>\n",
              "      <th>BCL10</th>\n",
              "      <th>BIRC2</th>\n",
              "      <th>BIRC3</th>\n",
              "      <th>BPIFB1</th>\n",
              "      <th>BTK</th>\n",
              "      <th>C1QBP</th>\n",
              "      <th>CACTIN</th>\n",
              "      <th>CASP8</th>\n",
              "      <th>CAV1</th>\n",
              "      <th>CD14</th>\n",
              "      <th>CD163</th>\n",
              "      <th>CD163L1</th>\n",
              "      <th>CD207</th>\n",
              "      <th>CD209</th>\n",
              "      <th>CD248</th>\n",
              "      <th>CD300A</th>\n",
              "      <th>CD300LF</th>\n",
              "      <th>CD302</th>\n",
              "      <th>CD36</th>\n",
              "      <th>CD5</th>\n",
              "      <th>CD5L</th>\n",
              "      <th>CD6</th>\n",
              "      <th>CD68</th>\n",
              "      <th>CD69</th>\n",
              "      <th>CD72</th>\n",
              "      <th>CD93</th>\n",
              "      <th>...</th>\n",
              "      <th>TLR5</th>\n",
              "      <th>TLR6</th>\n",
              "      <th>TLR7</th>\n",
              "      <th>TLR8</th>\n",
              "      <th>TLR9</th>\n",
              "      <th>TMEM173</th>\n",
              "      <th>TMPRSS13</th>\n",
              "      <th>TMPRSS15</th>\n",
              "      <th>TMPRSS2</th>\n",
              "      <th>TMPRSS3</th>\n",
              "      <th>TMPRSS4</th>\n",
              "      <th>TMPRSS5</th>\n",
              "      <th>TNFAIP3</th>\n",
              "      <th>TNIP1</th>\n",
              "      <th>TNIP2</th>\n",
              "      <th>TNIP3</th>\n",
              "      <th>TRAF3</th>\n",
              "      <th>TRAF6</th>\n",
              "      <th>TREML4</th>\n",
              "      <th>TRIL</th>\n",
              "      <th>TRIM15</th>\n",
              "      <th>TRIM5</th>\n",
              "      <th>TSPAN6</th>\n",
              "      <th>TYRO3</th>\n",
              "      <th>UBA52</th>\n",
              "      <th>UBB</th>\n",
              "      <th>UBC</th>\n",
              "      <th>UBE2D1</th>\n",
              "      <th>UBE2D2</th>\n",
              "      <th>UBE2D3</th>\n",
              "      <th>UBE2N</th>\n",
              "      <th>UBE2V1</th>\n",
              "      <th>UBQLN1</th>\n",
              "      <th>UFD1</th>\n",
              "      <th>UNC93B1</th>\n",
              "      <th>USP17L2</th>\n",
              "      <th>VCAN</th>\n",
              "      <th>WDFY1</th>\n",
              "      <th>XIAP</th>\n",
              "      <th>ZCCHC3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>13.6584</td>\n",
              "      <td>88.0205</td>\n",
              "      <td>402.5420</td>\n",
              "      <td>1424.2625</td>\n",
              "      <td>3.7940</td>\n",
              "      <td>1001.2330</td>\n",
              "      <td>508.0148</td>\n",
              "      <td>567.2010</td>\n",
              "      <td>13.6584</td>\n",
              "      <td>1.8970</td>\n",
              "      <td>1181.8268</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>33.3871</td>\n",
              "      <td>640.0455</td>\n",
              "      <td>1412.8806</td>\n",
              "      <td>685.1940</td>\n",
              "      <td>17.4523</td>\n",
              "      <td>79.6737</td>\n",
              "      <td>1546.6869</td>\n",
              "      <td>756.1415</td>\n",
              "      <td>100.9200</td>\n",
              "      <td>7964.3365</td>\n",
              "      <td>2125.3913</td>\n",
              "      <td>596.4147</td>\n",
              "      <td>48.5630</td>\n",
              "      <td>0.7588</td>\n",
              "      <td>284.5490</td>\n",
              "      <td>55.3922</td>\n",
              "      <td>194.6315</td>\n",
              "      <td>34.1459</td>\n",
              "      <td>134.7093</td>\n",
              "      <td>12.1408</td>\n",
              "      <td>33.7665</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>80.8119</td>\n",
              "      <td>1229.2516</td>\n",
              "      <td>117.9930</td>\n",
              "      <td>45.5278</td>\n",
              "      <td>544.0577</td>\n",
              "      <td>...</td>\n",
              "      <td>43.2514</td>\n",
              "      <td>6.4498</td>\n",
              "      <td>20.8669</td>\n",
              "      <td>36.4223</td>\n",
              "      <td>6.3587</td>\n",
              "      <td>1536.1851</td>\n",
              "      <td>240.9181</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>2.2764</td>\n",
              "      <td>943.5645</td>\n",
              "      <td>363.8433</td>\n",
              "      <td>3.7940</td>\n",
              "      <td>1055.8665</td>\n",
              "      <td>4627.5254</td>\n",
              "      <td>968.9842</td>\n",
              "      <td>9.4850</td>\n",
              "      <td>359.2905</td>\n",
              "      <td>174.1440</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>84.6059</td>\n",
              "      <td>7.2086</td>\n",
              "      <td>1142.5780</td>\n",
              "      <td>628.6636</td>\n",
              "      <td>272.0288</td>\n",
              "      <td>10215.3087</td>\n",
              "      <td>17134.4020</td>\n",
              "      <td>22436.4982</td>\n",
              "      <td>495.1152</td>\n",
              "      <td>2472.5410</td>\n",
              "      <td>6849.6633</td>\n",
              "      <td>2437.6363</td>\n",
              "      <td>3413.2107</td>\n",
              "      <td>4135.4453</td>\n",
              "      <td>950.0104</td>\n",
              "      <td>1434.1269</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>31.8695</td>\n",
              "      <td>923.0769</td>\n",
              "      <td>1046.0021</td>\n",
              "      <td>819.1217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>22.0844</td>\n",
              "      <td>86.3807</td>\n",
              "      <td>690.4861</td>\n",
              "      <td>2226.3285</td>\n",
              "      <td>20.4071</td>\n",
              "      <td>486.9744</td>\n",
              "      <td>1448.0640</td>\n",
              "      <td>651.9083</td>\n",
              "      <td>11.1820</td>\n",
              "      <td>9.5047</td>\n",
              "      <td>2046.0194</td>\n",
              "      <td>249.0782</td>\n",
              "      <td>21.8048</td>\n",
              "      <td>1139.4419</td>\n",
              "      <td>1194.5130</td>\n",
              "      <td>1214.9201</td>\n",
              "      <td>42873.3168</td>\n",
              "      <td>79.6715</td>\n",
              "      <td>1133.2387</td>\n",
              "      <td>481.3834</td>\n",
              "      <td>475.2334</td>\n",
              "      <td>3385.3388</td>\n",
              "      <td>668.6813</td>\n",
              "      <td>171.9227</td>\n",
              "      <td>79.1124</td>\n",
              "      <td>113.2174</td>\n",
              "      <td>90.8534</td>\n",
              "      <td>631.7808</td>\n",
              "      <td>127.1948</td>\n",
              "      <td>29.0731</td>\n",
              "      <td>256.8552</td>\n",
              "      <td>168.0090</td>\n",
              "      <td>102.8740</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>224.4779</td>\n",
              "      <td>4810.4797</td>\n",
              "      <td>138.0972</td>\n",
              "      <td>20.1275</td>\n",
              "      <td>708.0977</td>\n",
              "      <td>...</td>\n",
              "      <td>393.8846</td>\n",
              "      <td>131.1085</td>\n",
              "      <td>17.3320</td>\n",
              "      <td>16.4934</td>\n",
              "      <td>14.5449</td>\n",
              "      <td>1081.8548</td>\n",
              "      <td>1425.4205</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1006.3765</td>\n",
              "      <td>645.1992</td>\n",
              "      <td>7826.5342</td>\n",
              "      <td>3.6341</td>\n",
              "      <td>1994.3028</td>\n",
              "      <td>2630.8359</td>\n",
              "      <td>889.8046</td>\n",
              "      <td>762.3302</td>\n",
              "      <td>229.2302</td>\n",
              "      <td>183.1046</td>\n",
              "      <td>1.1182</td>\n",
              "      <td>119.9265</td>\n",
              "      <td>7.5478</td>\n",
              "      <td>575.9856</td>\n",
              "      <td>953.5417</td>\n",
              "      <td>392.2073</td>\n",
              "      <td>6344.0857</td>\n",
              "      <td>10287.1248</td>\n",
              "      <td>38975.0055</td>\n",
              "      <td>499.8337</td>\n",
              "      <td>1633.4050</td>\n",
              "      <td>6070.1277</td>\n",
              "      <td>2153.9253</td>\n",
              "      <td>1817.3762</td>\n",
              "      <td>3426.4325</td>\n",
              "      <td>1380.1196</td>\n",
              "      <td>815.7241</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1651.0166</td>\n",
              "      <td>1473.7825</td>\n",
              "      <td>1618.8684</td>\n",
              "      <td>615.8465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>32.7422</td>\n",
              "      <td>53.0820</td>\n",
              "      <td>128.4882</td>\n",
              "      <td>1073.0497</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>654.8431</td>\n",
              "      <td>492.6206</td>\n",
              "      <td>512.9604</td>\n",
              "      <td>13.8906</td>\n",
              "      <td>1.4883</td>\n",
              "      <td>1530.4477</td>\n",
              "      <td>3.9687</td>\n",
              "      <td>3.9687</td>\n",
              "      <td>739.1790</td>\n",
              "      <td>41465.9556</td>\n",
              "      <td>22257.7205</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>21.8281</td>\n",
              "      <td>2671.1348</td>\n",
              "      <td>731.2415</td>\n",
              "      <td>232.1716</td>\n",
              "      <td>5547.8110</td>\n",
              "      <td>966.3897</td>\n",
              "      <td>342.8004</td>\n",
              "      <td>37.7031</td>\n",
              "      <td>5.9531</td>\n",
              "      <td>66.9726</td>\n",
              "      <td>300.6325</td>\n",
              "      <td>185.5389</td>\n",
              "      <td>22.3242</td>\n",
              "      <td>237.5642</td>\n",
              "      <td>20.8359</td>\n",
              "      <td>32.2461</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>66.4765</td>\n",
              "      <td>2461.1187</td>\n",
              "      <td>28.2773</td>\n",
              "      <td>31.2539</td>\n",
              "      <td>220.2654</td>\n",
              "      <td>...</td>\n",
              "      <td>25.3008</td>\n",
              "      <td>7.9375</td>\n",
              "      <td>20.3398</td>\n",
              "      <td>11.4101</td>\n",
              "      <td>7.8184</td>\n",
              "      <td>736.6985</td>\n",
              "      <td>96.7382</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>2.4805</td>\n",
              "      <td>54.0742</td>\n",
              "      <td>547.1909</td>\n",
              "      <td>0.4961</td>\n",
              "      <td>3035.5947</td>\n",
              "      <td>2523.1303</td>\n",
              "      <td>561.5776</td>\n",
              "      <td>5.9531</td>\n",
              "      <td>390.4254</td>\n",
              "      <td>209.8475</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>60.5234</td>\n",
              "      <td>76.8945</td>\n",
              "      <td>766.6476</td>\n",
              "      <td>1379.1393</td>\n",
              "      <td>320.4762</td>\n",
              "      <td>13086.9403</td>\n",
              "      <td>16118.0702</td>\n",
              "      <td>42394.1461</td>\n",
              "      <td>557.1127</td>\n",
              "      <td>1415.3541</td>\n",
              "      <td>3512.3403</td>\n",
              "      <td>3122.4110</td>\n",
              "      <td>3720.6995</td>\n",
              "      <td>3000.3721</td>\n",
              "      <td>2681.3791</td>\n",
              "      <td>1429.2447</td>\n",
              "      <td>0.4961</td>\n",
              "      <td>693.5384</td>\n",
              "      <td>477.2417</td>\n",
              "      <td>1064.6161</td>\n",
              "      <td>804.1672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>22.7975</td>\n",
              "      <td>42.1175</td>\n",
              "      <td>634.0804</td>\n",
              "      <td>3285.5487</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>574.9614</td>\n",
              "      <td>513.9104</td>\n",
              "      <td>1033.2303</td>\n",
              "      <td>31.6847</td>\n",
              "      <td>3.8640</td>\n",
              "      <td>706.3369</td>\n",
              "      <td>0.7728</td>\n",
              "      <td>14.6832</td>\n",
              "      <td>1635.6260</td>\n",
              "      <td>540.5719</td>\n",
              "      <td>353.1685</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>295.2087</td>\n",
              "      <td>1844.5750</td>\n",
              "      <td>496.5224</td>\n",
              "      <td>977.2025</td>\n",
              "      <td>1003.4776</td>\n",
              "      <td>3450.5410</td>\n",
              "      <td>907.6507</td>\n",
              "      <td>3.4776</td>\n",
              "      <td>121.3292</td>\n",
              "      <td>62.5966</td>\n",
              "      <td>154.5595</td>\n",
              "      <td>412.6739</td>\n",
              "      <td>116.6924</td>\n",
              "      <td>255.4096</td>\n",
              "      <td>4506.1824</td>\n",
              "      <td>175.4250</td>\n",
              "      <td>1.1592</td>\n",
              "      <td>225.2705</td>\n",
              "      <td>7845.8269</td>\n",
              "      <td>139.8764</td>\n",
              "      <td>247.6816</td>\n",
              "      <td>659.9691</td>\n",
              "      <td>...</td>\n",
              "      <td>814.1422</td>\n",
              "      <td>39.7991</td>\n",
              "      <td>205.9505</td>\n",
              "      <td>79.9845</td>\n",
              "      <td>10.2473</td>\n",
              "      <td>1145.2859</td>\n",
              "      <td>175.4250</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>707.1097</td>\n",
              "      <td>66.4606</td>\n",
              "      <td>4673.1066</td>\n",
              "      <td>0.3864</td>\n",
              "      <td>1360.5100</td>\n",
              "      <td>2974.8841</td>\n",
              "      <td>1202.4730</td>\n",
              "      <td>49.4590</td>\n",
              "      <td>518.9335</td>\n",
              "      <td>158.8099</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>316.8470</td>\n",
              "      <td>0.3864</td>\n",
              "      <td>711.2674</td>\n",
              "      <td>1255.0232</td>\n",
              "      <td>858.9645</td>\n",
              "      <td>8149.1499</td>\n",
              "      <td>14166.5379</td>\n",
              "      <td>72170.7071</td>\n",
              "      <td>363.6012</td>\n",
              "      <td>2251.9320</td>\n",
              "      <td>7059.5054</td>\n",
              "      <td>2663.4467</td>\n",
              "      <td>3217.9212</td>\n",
              "      <td>4131.3756</td>\n",
              "      <td>1178.8640</td>\n",
              "      <td>1018.5471</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>675.0386</td>\n",
              "      <td>1596.5997</td>\n",
              "      <td>1666.1515</td>\n",
              "      <td>402.6275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2.5000</td>\n",
              "      <td>134.0625</td>\n",
              "      <td>532.1875</td>\n",
              "      <td>936.5625</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1933.4375</td>\n",
              "      <td>679.3750</td>\n",
              "      <td>1175.6250</td>\n",
              "      <td>6.5625</td>\n",
              "      <td>18.7500</td>\n",
              "      <td>1483.4375</td>\n",
              "      <td>0.3125</td>\n",
              "      <td>8.1250</td>\n",
              "      <td>1490.3125</td>\n",
              "      <td>740.3125</td>\n",
              "      <td>401.2500</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>104.3750</td>\n",
              "      <td>1046.2969</td>\n",
              "      <td>387.8125</td>\n",
              "      <td>775.3125</td>\n",
              "      <td>693.7500</td>\n",
              "      <td>1370.0000</td>\n",
              "      <td>118.4375</td>\n",
              "      <td>15.6250</td>\n",
              "      <td>389.6875</td>\n",
              "      <td>32.1875</td>\n",
              "      <td>29.3750</td>\n",
              "      <td>308.7500</td>\n",
              "      <td>79.3750</td>\n",
              "      <td>205.8594</td>\n",
              "      <td>13.1250</td>\n",
              "      <td>106.5625</td>\n",
              "      <td>0.6250</td>\n",
              "      <td>215.0000</td>\n",
              "      <td>1057.1875</td>\n",
              "      <td>1046.8750</td>\n",
              "      <td>78.1250</td>\n",
              "      <td>249.0625</td>\n",
              "      <td>...</td>\n",
              "      <td>345.0000</td>\n",
              "      <td>33.7500</td>\n",
              "      <td>61.5625</td>\n",
              "      <td>25.6250</td>\n",
              "      <td>13.0813</td>\n",
              "      <td>851.5625</td>\n",
              "      <td>399.6875</td>\n",
              "      <td>0.3125</td>\n",
              "      <td>185.9375</td>\n",
              "      <td>145.6250</td>\n",
              "      <td>2347.8125</td>\n",
              "      <td>4.3750</td>\n",
              "      <td>725.6250</td>\n",
              "      <td>3347.1875</td>\n",
              "      <td>689.6875</td>\n",
              "      <td>24.3750</td>\n",
              "      <td>201.5625</td>\n",
              "      <td>71.5625</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>667.8125</td>\n",
              "      <td>1.5625</td>\n",
              "      <td>387.5906</td>\n",
              "      <td>1559.6875</td>\n",
              "      <td>348.7500</td>\n",
              "      <td>8364.3750</td>\n",
              "      <td>5850.0000</td>\n",
              "      <td>46026.2500</td>\n",
              "      <td>598.1250</td>\n",
              "      <td>4762.1875</td>\n",
              "      <td>4748.7500</td>\n",
              "      <td>1898.4375</td>\n",
              "      <td>3009.1375</td>\n",
              "      <td>2549.3750</td>\n",
              "      <td>1293.1156</td>\n",
              "      <td>744.6875</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>69.0625</td>\n",
              "      <td>1651.5625</td>\n",
              "      <td>1250.3125</td>\n",
              "      <td>982.5000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 319 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Labels     ACAN      AGER  ...      WDFY1       XIAP    ZCCHC3\n",
              "0       0  13.6584   88.0205  ...   923.0769  1046.0021  819.1217\n",
              "1       0  22.0844   86.3807  ...  1473.7825  1618.8684  615.8465\n",
              "2       0  32.7422   53.0820  ...   477.2417  1064.6161  804.1672\n",
              "3       0  22.7975   42.1175  ...  1596.5997  1666.1515  402.6275\n",
              "4       0   2.5000  134.0625  ...  1651.5625  1250.3125  982.5000\n",
              "\n",
              "[5 rows x 319 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5jVQkuNmZyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c1339a3-1d09-45b4-c1d3-ab5eca53e52c"
      },
      "source": [
        "#Print testing data \n",
        "print(\"Fews rows of testing data\")\n",
        "print(testing_data.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fews rows of testing data\n",
            "     ID      ACAN      AGER  ...      WDFY1       XIAP    ZCCHC3\n",
            "0  1001   91.3265  132.5226  ...   995.6570  1219.0098  840.7991\n",
            "1  1002   17.5971  183.2524  ...  1529.7330  1083.1311  510.3155\n",
            "2  1003  120.9309  221.8022  ...  1156.5812   710.6842  689.4783\n",
            "3  1004   12.0930   41.8605  ...  1065.1163  1385.1163  563.7209\n",
            "4  1005   84.0622   34.4995  ...   581.6327   878.5228  607.8717\n",
            "\n",
            "[5 rows x 319 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biPISfAmmZ7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ef4327-13ee-4d39-c865-801fb2a0271e"
      },
      "source": [
        "#prints training data info\n",
        "training_data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 242 entries, 0 to 241\n",
            "Columns: 319 entries, Labels to ZCCHC3\n",
            "dtypes: float64(318), int64(1)\n",
            "memory usage: 603.2 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kih_nmWImaBx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "a705613b-cf47-4711-b23d-69e528656d0d"
      },
      "source": [
        "#Again print training data\n",
        "training_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labels</th>\n",
              "      <th>ACAN</th>\n",
              "      <th>AGER</th>\n",
              "      <th>ALPK1</th>\n",
              "      <th>ANKRD17</th>\n",
              "      <th>APOB</th>\n",
              "      <th>APPL1</th>\n",
              "      <th>APPL2</th>\n",
              "      <th>ARRB2</th>\n",
              "      <th>ASGR1</th>\n",
              "      <th>ASGR2</th>\n",
              "      <th>ATRN</th>\n",
              "      <th>ATRNL1</th>\n",
              "      <th>BCAN</th>\n",
              "      <th>BCL10</th>\n",
              "      <th>BIRC2</th>\n",
              "      <th>BIRC3</th>\n",
              "      <th>BPIFB1</th>\n",
              "      <th>BTK</th>\n",
              "      <th>C1QBP</th>\n",
              "      <th>CACTIN</th>\n",
              "      <th>CASP8</th>\n",
              "      <th>CAV1</th>\n",
              "      <th>CD14</th>\n",
              "      <th>CD163</th>\n",
              "      <th>CD163L1</th>\n",
              "      <th>CD207</th>\n",
              "      <th>CD209</th>\n",
              "      <th>CD248</th>\n",
              "      <th>CD300A</th>\n",
              "      <th>CD300LF</th>\n",
              "      <th>CD302</th>\n",
              "      <th>CD36</th>\n",
              "      <th>CD5</th>\n",
              "      <th>CD5L</th>\n",
              "      <th>CD6</th>\n",
              "      <th>CD68</th>\n",
              "      <th>CD69</th>\n",
              "      <th>CD72</th>\n",
              "      <th>CD93</th>\n",
              "      <th>...</th>\n",
              "      <th>TLR5</th>\n",
              "      <th>TLR6</th>\n",
              "      <th>TLR7</th>\n",
              "      <th>TLR8</th>\n",
              "      <th>TLR9</th>\n",
              "      <th>TMEM173</th>\n",
              "      <th>TMPRSS13</th>\n",
              "      <th>TMPRSS15</th>\n",
              "      <th>TMPRSS2</th>\n",
              "      <th>TMPRSS3</th>\n",
              "      <th>TMPRSS4</th>\n",
              "      <th>TMPRSS5</th>\n",
              "      <th>TNFAIP3</th>\n",
              "      <th>TNIP1</th>\n",
              "      <th>TNIP2</th>\n",
              "      <th>TNIP3</th>\n",
              "      <th>TRAF3</th>\n",
              "      <th>TRAF6</th>\n",
              "      <th>TREML4</th>\n",
              "      <th>TRIL</th>\n",
              "      <th>TRIM15</th>\n",
              "      <th>TRIM5</th>\n",
              "      <th>TSPAN6</th>\n",
              "      <th>TYRO3</th>\n",
              "      <th>UBA52</th>\n",
              "      <th>UBB</th>\n",
              "      <th>UBC</th>\n",
              "      <th>UBE2D1</th>\n",
              "      <th>UBE2D2</th>\n",
              "      <th>UBE2D3</th>\n",
              "      <th>UBE2N</th>\n",
              "      <th>UBE2V1</th>\n",
              "      <th>UBQLN1</th>\n",
              "      <th>UFD1</th>\n",
              "      <th>UNC93B1</th>\n",
              "      <th>USP17L2</th>\n",
              "      <th>VCAN</th>\n",
              "      <th>WDFY1</th>\n",
              "      <th>XIAP</th>\n",
              "      <th>ZCCHC3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>13.6584</td>\n",
              "      <td>88.0205</td>\n",
              "      <td>402.5420</td>\n",
              "      <td>1424.2625</td>\n",
              "      <td>3.7940</td>\n",
              "      <td>1001.2330</td>\n",
              "      <td>508.0148</td>\n",
              "      <td>567.2010</td>\n",
              "      <td>13.6584</td>\n",
              "      <td>1.8970</td>\n",
              "      <td>1181.8268</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>33.3871</td>\n",
              "      <td>640.0455</td>\n",
              "      <td>1412.8806</td>\n",
              "      <td>685.1940</td>\n",
              "      <td>17.4523</td>\n",
              "      <td>79.6737</td>\n",
              "      <td>1546.6869</td>\n",
              "      <td>756.1415</td>\n",
              "      <td>100.9200</td>\n",
              "      <td>7964.3365</td>\n",
              "      <td>2125.3913</td>\n",
              "      <td>596.4147</td>\n",
              "      <td>48.5630</td>\n",
              "      <td>0.7588</td>\n",
              "      <td>284.5490</td>\n",
              "      <td>55.3922</td>\n",
              "      <td>194.6315</td>\n",
              "      <td>34.1459</td>\n",
              "      <td>134.7093</td>\n",
              "      <td>12.1408</td>\n",
              "      <td>33.7665</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>80.8119</td>\n",
              "      <td>1229.2516</td>\n",
              "      <td>117.9930</td>\n",
              "      <td>45.5278</td>\n",
              "      <td>544.0577</td>\n",
              "      <td>...</td>\n",
              "      <td>43.2514</td>\n",
              "      <td>6.4498</td>\n",
              "      <td>20.8669</td>\n",
              "      <td>36.4223</td>\n",
              "      <td>6.3587</td>\n",
              "      <td>1536.1851</td>\n",
              "      <td>240.9181</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>2.2764</td>\n",
              "      <td>943.5645</td>\n",
              "      <td>363.8433</td>\n",
              "      <td>3.7940</td>\n",
              "      <td>1055.8665</td>\n",
              "      <td>4627.5254</td>\n",
              "      <td>968.9842</td>\n",
              "      <td>9.4850</td>\n",
              "      <td>359.2905</td>\n",
              "      <td>174.1440</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>84.6059</td>\n",
              "      <td>7.2086</td>\n",
              "      <td>1142.5780</td>\n",
              "      <td>628.6636</td>\n",
              "      <td>272.0288</td>\n",
              "      <td>10215.3087</td>\n",
              "      <td>17134.4020</td>\n",
              "      <td>22436.4982</td>\n",
              "      <td>495.1152</td>\n",
              "      <td>2472.5410</td>\n",
              "      <td>6849.6633</td>\n",
              "      <td>2437.6363</td>\n",
              "      <td>3413.2107</td>\n",
              "      <td>4135.4453</td>\n",
              "      <td>950.0104</td>\n",
              "      <td>1434.1269</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>31.8695</td>\n",
              "      <td>923.0769</td>\n",
              "      <td>1046.0021</td>\n",
              "      <td>819.1217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>22.0844</td>\n",
              "      <td>86.3807</td>\n",
              "      <td>690.4861</td>\n",
              "      <td>2226.3285</td>\n",
              "      <td>20.4071</td>\n",
              "      <td>486.9744</td>\n",
              "      <td>1448.0640</td>\n",
              "      <td>651.9083</td>\n",
              "      <td>11.1820</td>\n",
              "      <td>9.5047</td>\n",
              "      <td>2046.0194</td>\n",
              "      <td>249.0782</td>\n",
              "      <td>21.8048</td>\n",
              "      <td>1139.4419</td>\n",
              "      <td>1194.5130</td>\n",
              "      <td>1214.9201</td>\n",
              "      <td>42873.3168</td>\n",
              "      <td>79.6715</td>\n",
              "      <td>1133.2387</td>\n",
              "      <td>481.3834</td>\n",
              "      <td>475.2334</td>\n",
              "      <td>3385.3388</td>\n",
              "      <td>668.6813</td>\n",
              "      <td>171.9227</td>\n",
              "      <td>79.1124</td>\n",
              "      <td>113.2174</td>\n",
              "      <td>90.8534</td>\n",
              "      <td>631.7808</td>\n",
              "      <td>127.1948</td>\n",
              "      <td>29.0731</td>\n",
              "      <td>256.8552</td>\n",
              "      <td>168.0090</td>\n",
              "      <td>102.8740</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>224.4779</td>\n",
              "      <td>4810.4797</td>\n",
              "      <td>138.0972</td>\n",
              "      <td>20.1275</td>\n",
              "      <td>708.0977</td>\n",
              "      <td>...</td>\n",
              "      <td>393.8846</td>\n",
              "      <td>131.1085</td>\n",
              "      <td>17.3320</td>\n",
              "      <td>16.4934</td>\n",
              "      <td>14.5449</td>\n",
              "      <td>1081.8548</td>\n",
              "      <td>1425.4205</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1006.3765</td>\n",
              "      <td>645.1992</td>\n",
              "      <td>7826.5342</td>\n",
              "      <td>3.6341</td>\n",
              "      <td>1994.3028</td>\n",
              "      <td>2630.8359</td>\n",
              "      <td>889.8046</td>\n",
              "      <td>762.3302</td>\n",
              "      <td>229.2302</td>\n",
              "      <td>183.1046</td>\n",
              "      <td>1.1182</td>\n",
              "      <td>119.9265</td>\n",
              "      <td>7.5478</td>\n",
              "      <td>575.9856</td>\n",
              "      <td>953.5417</td>\n",
              "      <td>392.2073</td>\n",
              "      <td>6344.0857</td>\n",
              "      <td>10287.1248</td>\n",
              "      <td>38975.0055</td>\n",
              "      <td>499.8337</td>\n",
              "      <td>1633.4050</td>\n",
              "      <td>6070.1277</td>\n",
              "      <td>2153.9253</td>\n",
              "      <td>1817.3762</td>\n",
              "      <td>3426.4325</td>\n",
              "      <td>1380.1196</td>\n",
              "      <td>815.7241</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1651.0166</td>\n",
              "      <td>1473.7825</td>\n",
              "      <td>1618.8684</td>\n",
              "      <td>615.8465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>32.7422</td>\n",
              "      <td>53.0820</td>\n",
              "      <td>128.4882</td>\n",
              "      <td>1073.0497</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>654.8431</td>\n",
              "      <td>492.6206</td>\n",
              "      <td>512.9604</td>\n",
              "      <td>13.8906</td>\n",
              "      <td>1.4883</td>\n",
              "      <td>1530.4477</td>\n",
              "      <td>3.9687</td>\n",
              "      <td>3.9687</td>\n",
              "      <td>739.1790</td>\n",
              "      <td>41465.9556</td>\n",
              "      <td>22257.7205</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>21.8281</td>\n",
              "      <td>2671.1348</td>\n",
              "      <td>731.2415</td>\n",
              "      <td>232.1716</td>\n",
              "      <td>5547.8110</td>\n",
              "      <td>966.3897</td>\n",
              "      <td>342.8004</td>\n",
              "      <td>37.7031</td>\n",
              "      <td>5.9531</td>\n",
              "      <td>66.9726</td>\n",
              "      <td>300.6325</td>\n",
              "      <td>185.5389</td>\n",
              "      <td>22.3242</td>\n",
              "      <td>237.5642</td>\n",
              "      <td>20.8359</td>\n",
              "      <td>32.2461</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>66.4765</td>\n",
              "      <td>2461.1187</td>\n",
              "      <td>28.2773</td>\n",
              "      <td>31.2539</td>\n",
              "      <td>220.2654</td>\n",
              "      <td>...</td>\n",
              "      <td>25.3008</td>\n",
              "      <td>7.9375</td>\n",
              "      <td>20.3398</td>\n",
              "      <td>11.4101</td>\n",
              "      <td>7.8184</td>\n",
              "      <td>736.6985</td>\n",
              "      <td>96.7382</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>2.4805</td>\n",
              "      <td>54.0742</td>\n",
              "      <td>547.1909</td>\n",
              "      <td>0.4961</td>\n",
              "      <td>3035.5947</td>\n",
              "      <td>2523.1303</td>\n",
              "      <td>561.5776</td>\n",
              "      <td>5.9531</td>\n",
              "      <td>390.4254</td>\n",
              "      <td>209.8475</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>60.5234</td>\n",
              "      <td>76.8945</td>\n",
              "      <td>766.6476</td>\n",
              "      <td>1379.1393</td>\n",
              "      <td>320.4762</td>\n",
              "      <td>13086.9403</td>\n",
              "      <td>16118.0702</td>\n",
              "      <td>42394.1461</td>\n",
              "      <td>557.1127</td>\n",
              "      <td>1415.3541</td>\n",
              "      <td>3512.3403</td>\n",
              "      <td>3122.4110</td>\n",
              "      <td>3720.6995</td>\n",
              "      <td>3000.3721</td>\n",
              "      <td>2681.3791</td>\n",
              "      <td>1429.2447</td>\n",
              "      <td>0.4961</td>\n",
              "      <td>693.5384</td>\n",
              "      <td>477.2417</td>\n",
              "      <td>1064.6161</td>\n",
              "      <td>804.1672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>22.7975</td>\n",
              "      <td>42.1175</td>\n",
              "      <td>634.0804</td>\n",
              "      <td>3285.5487</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>574.9614</td>\n",
              "      <td>513.9104</td>\n",
              "      <td>1033.2303</td>\n",
              "      <td>31.6847</td>\n",
              "      <td>3.8640</td>\n",
              "      <td>706.3369</td>\n",
              "      <td>0.7728</td>\n",
              "      <td>14.6832</td>\n",
              "      <td>1635.6260</td>\n",
              "      <td>540.5719</td>\n",
              "      <td>353.1685</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>295.2087</td>\n",
              "      <td>1844.5750</td>\n",
              "      <td>496.5224</td>\n",
              "      <td>977.2025</td>\n",
              "      <td>1003.4776</td>\n",
              "      <td>3450.5410</td>\n",
              "      <td>907.6507</td>\n",
              "      <td>3.4776</td>\n",
              "      <td>121.3292</td>\n",
              "      <td>62.5966</td>\n",
              "      <td>154.5595</td>\n",
              "      <td>412.6739</td>\n",
              "      <td>116.6924</td>\n",
              "      <td>255.4096</td>\n",
              "      <td>4506.1824</td>\n",
              "      <td>175.4250</td>\n",
              "      <td>1.1592</td>\n",
              "      <td>225.2705</td>\n",
              "      <td>7845.8269</td>\n",
              "      <td>139.8764</td>\n",
              "      <td>247.6816</td>\n",
              "      <td>659.9691</td>\n",
              "      <td>...</td>\n",
              "      <td>814.1422</td>\n",
              "      <td>39.7991</td>\n",
              "      <td>205.9505</td>\n",
              "      <td>79.9845</td>\n",
              "      <td>10.2473</td>\n",
              "      <td>1145.2859</td>\n",
              "      <td>175.4250</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>707.1097</td>\n",
              "      <td>66.4606</td>\n",
              "      <td>4673.1066</td>\n",
              "      <td>0.3864</td>\n",
              "      <td>1360.5100</td>\n",
              "      <td>2974.8841</td>\n",
              "      <td>1202.4730</td>\n",
              "      <td>49.4590</td>\n",
              "      <td>518.9335</td>\n",
              "      <td>158.8099</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>316.8470</td>\n",
              "      <td>0.3864</td>\n",
              "      <td>711.2674</td>\n",
              "      <td>1255.0232</td>\n",
              "      <td>858.9645</td>\n",
              "      <td>8149.1499</td>\n",
              "      <td>14166.5379</td>\n",
              "      <td>72170.7071</td>\n",
              "      <td>363.6012</td>\n",
              "      <td>2251.9320</td>\n",
              "      <td>7059.5054</td>\n",
              "      <td>2663.4467</td>\n",
              "      <td>3217.9212</td>\n",
              "      <td>4131.3756</td>\n",
              "      <td>1178.8640</td>\n",
              "      <td>1018.5471</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>675.0386</td>\n",
              "      <td>1596.5997</td>\n",
              "      <td>1666.1515</td>\n",
              "      <td>402.6275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2.5000</td>\n",
              "      <td>134.0625</td>\n",
              "      <td>532.1875</td>\n",
              "      <td>936.5625</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1933.4375</td>\n",
              "      <td>679.3750</td>\n",
              "      <td>1175.6250</td>\n",
              "      <td>6.5625</td>\n",
              "      <td>18.7500</td>\n",
              "      <td>1483.4375</td>\n",
              "      <td>0.3125</td>\n",
              "      <td>8.1250</td>\n",
              "      <td>1490.3125</td>\n",
              "      <td>740.3125</td>\n",
              "      <td>401.2500</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>104.3750</td>\n",
              "      <td>1046.2969</td>\n",
              "      <td>387.8125</td>\n",
              "      <td>775.3125</td>\n",
              "      <td>693.7500</td>\n",
              "      <td>1370.0000</td>\n",
              "      <td>118.4375</td>\n",
              "      <td>15.6250</td>\n",
              "      <td>389.6875</td>\n",
              "      <td>32.1875</td>\n",
              "      <td>29.3750</td>\n",
              "      <td>308.7500</td>\n",
              "      <td>79.3750</td>\n",
              "      <td>205.8594</td>\n",
              "      <td>13.1250</td>\n",
              "      <td>106.5625</td>\n",
              "      <td>0.6250</td>\n",
              "      <td>215.0000</td>\n",
              "      <td>1057.1875</td>\n",
              "      <td>1046.8750</td>\n",
              "      <td>78.1250</td>\n",
              "      <td>249.0625</td>\n",
              "      <td>...</td>\n",
              "      <td>345.0000</td>\n",
              "      <td>33.7500</td>\n",
              "      <td>61.5625</td>\n",
              "      <td>25.6250</td>\n",
              "      <td>13.0813</td>\n",
              "      <td>851.5625</td>\n",
              "      <td>399.6875</td>\n",
              "      <td>0.3125</td>\n",
              "      <td>185.9375</td>\n",
              "      <td>145.6250</td>\n",
              "      <td>2347.8125</td>\n",
              "      <td>4.3750</td>\n",
              "      <td>725.6250</td>\n",
              "      <td>3347.1875</td>\n",
              "      <td>689.6875</td>\n",
              "      <td>24.3750</td>\n",
              "      <td>201.5625</td>\n",
              "      <td>71.5625</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>667.8125</td>\n",
              "      <td>1.5625</td>\n",
              "      <td>387.5906</td>\n",
              "      <td>1559.6875</td>\n",
              "      <td>348.7500</td>\n",
              "      <td>8364.3750</td>\n",
              "      <td>5850.0000</td>\n",
              "      <td>46026.2500</td>\n",
              "      <td>598.1250</td>\n",
              "      <td>4762.1875</td>\n",
              "      <td>4748.7500</td>\n",
              "      <td>1898.4375</td>\n",
              "      <td>3009.1375</td>\n",
              "      <td>2549.3750</td>\n",
              "      <td>1293.1156</td>\n",
              "      <td>744.6875</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>69.0625</td>\n",
              "      <td>1651.5625</td>\n",
              "      <td>1250.3125</td>\n",
              "      <td>982.5000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 319 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Labels     ACAN      AGER  ...      WDFY1       XIAP    ZCCHC3\n",
              "0       0  13.6584   88.0205  ...   923.0769  1046.0021  819.1217\n",
              "1       0  22.0844   86.3807  ...  1473.7825  1618.8684  615.8465\n",
              "2       0  32.7422   53.0820  ...   477.2417  1064.6161  804.1672\n",
              "3       0  22.7975   42.1175  ...  1596.5997  1666.1515  402.6275\n",
              "4       0   2.5000  134.0625  ...  1651.5625  1250.3125  982.5000\n",
              "\n",
              "[5 rows x 319 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwxD3722maHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb2d3a7-cf8f-4068-e50f-3d773a45931d"
      },
      "source": [
        "# Count the number of samples for eacch class\n",
        "print('1 : High Risk Cancer')\n",
        "print('0 : Low Risk Cancer\\n')\n",
        "print(training_data['Labels'].value_counts())\n",
        "count_early, count_late = training_data['Labels'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 : High Risk Cancer\n",
            "0 : Low Risk Cancer\n",
            "\n",
            "1    121\n",
            "0    121\n",
            "Name: Labels, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBV0xci_GMze"
      },
      "source": [
        "# Removing ID from the table\n",
        "\n",
        "X_test = testing_data.drop(['ID'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M_Dw8gumaLy"
      },
      "source": [
        "# Extract Labels and Data separatly from training_data\n",
        "\n",
        "X_train = training_data.drop('Labels',axis = 1)\n",
        "Y_train = training_data['Labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgDHGG-MEaIy"
      },
      "source": [
        "Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gXXJPnrmaF9"
      },
      "source": [
        "#scaling of the data using min-max scaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "sc = MinMaxScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYpzpD14GsxP"
      },
      "source": [
        "Splitting the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROy4vPgzmZ_y"
      },
      "source": [
        "# Given data is split into 70% training data where the model will be trained and remaining 30% validation data.\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_train,Y_train, test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcPYgcRB9sxj"
      },
      "source": [
        "Feature Reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42ULOiHk9rhD"
      },
      "source": [
        "# Features are reduced from 318 to lesser features using Principal Component Analysis (PCA)\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alu-5t8n9u8_"
      },
      "source": [
        "# Reducing the features to 125\n",
        "pca = PCA(n_components=125)\n",
        "\n",
        "x_train_pca = pca.fit_transform(x_train)\n",
        "x_test_pca = pca.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYZ7PRIEaYgo"
      },
      "source": [
        "CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Th2wY-vaXBk"
      },
      "source": [
        "# Importing all the required keras and tensorflow \n",
        "\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.python.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvRgpmVoaXHa"
      },
      "source": [
        "#Insializing the layers required for the model\n",
        "\n",
        "model = Sequential([\n",
        "                    Dense(16, input_shape = (169, 318), activation='relu'),\n",
        "                    Dense(32, activation = 'relu'),\n",
        "                    Dense(2, activation = 'softmax')\n",
        "                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msw7rHLtaXLS",
        "outputId": "ae310bd4-caca-4a13-c061-5076b1caaa3b"
      },
      "source": [
        "# Summanry of the model layers are mentioned here :\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 169, 16)           5104      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 169, 32)           544       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 169, 2)            66        \n",
            "=================================================================\n",
            "Total params: 5,714\n",
            "Trainable params: 5,714\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXzRClhVaXQm"
      },
      "source": [
        "#compile the model in such a way that folowing matrix is followed \n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK7Fe73naXWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13bb3528-6dd7-4b47-e4e2-eee5b9862a79"
      },
      "source": [
        "# Model is being trained with the splitted train and test data\n",
        "model.fit(x_train,y_train,batch_size=10, epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 169, 318) for input Tensor(\"dense_input:0\", shape=(None, 169, 318), dtype=float32), but it was called on an input with incompatible shape (None, 318).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 169, 318) for input Tensor(\"dense_input:0\", shape=(None, 169, 318), dtype=float32), but it was called on an input with incompatible shape (None, 318).\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.7047 - accuracy: 0.4142\n",
            "Epoch 2/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6863 - accuracy: 0.5976\n",
            "Epoch 3/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6789 - accuracy: 0.6154\n",
            "Epoch 4/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6718 - accuracy: 0.5976\n",
            "Epoch 5/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6648 - accuracy: 0.6095\n",
            "Epoch 6/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6543 - accuracy: 0.6331\n",
            "Epoch 7/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6408 - accuracy: 0.6805\n",
            "Epoch 8/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6223 - accuracy: 0.7041\n",
            "Epoch 9/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6090 - accuracy: 0.7219\n",
            "Epoch 10/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5768 - accuracy: 0.7929\n",
            "Epoch 11/50\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.5418 - accuracy: 0.8047\n",
            "Epoch 12/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.8047\n",
            "Epoch 13/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.8047\n",
            "Epoch 14/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.8935\n",
            "Epoch 15/50\n",
            "17/17 [==============================] - 0s 3ms/step - loss: 0.3986 - accuracy: 0.8639\n",
            "Epoch 16/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3834 - accuracy: 0.8876\n",
            "Epoch 17/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3561 - accuracy: 0.8994\n",
            "Epoch 18/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3178 - accuracy: 0.9112\n",
            "Epoch 19/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2854 - accuracy: 0.9172\n",
            "Epoch 20/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.9349\n",
            "Epoch 21/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2572 - accuracy: 0.9408\n",
            "Epoch 22/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2279 - accuracy: 0.9408\n",
            "Epoch 23/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2144 - accuracy: 0.9527\n",
            "Epoch 24/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1896 - accuracy: 0.9704\n",
            "Epoch 25/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1747 - accuracy: 0.9645\n",
            "Epoch 26/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1687 - accuracy: 0.9586\n",
            "Epoch 27/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1505 - accuracy: 0.9704\n",
            "Epoch 28/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1449 - accuracy: 0.9704\n",
            "Epoch 29/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1470 - accuracy: 0.9586\n",
            "Epoch 30/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1194 - accuracy: 0.9763\n",
            "Epoch 31/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1021 - accuracy: 0.9822\n",
            "Epoch 32/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0943 - accuracy: 0.9882\n",
            "Epoch 33/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0832 - accuracy: 0.9882\n",
            "Epoch 34/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.9882\n",
            "Epoch 35/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0653 - accuracy: 0.9941\n",
            "Epoch 36/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0576 - accuracy: 0.9941\n",
            "Epoch 37/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0536 - accuracy: 0.9941\n",
            "Epoch 38/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0470 - accuracy: 0.9941\n",
            "Epoch 39/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0425 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0495 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0348 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0338 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0158 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0132 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8679911828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BjX2ABTaXco",
        "outputId": "8f01ca18-1845-4879-f27e-a8da515693c0"
      },
      "source": [
        "# Produces list of prediction that of validation data\n",
        "model.predict_classes(x_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-25-2415c00a9d46>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 169, 318) for input Tensor(\"dense_input:0\", shape=(None, 169, 318), dtype=float32), but it was called on an input with incompatible shape (None, 318).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wja3LgL1aXi2",
        "outputId": "7bbde51e-985e-4e0b-e507-9a177391464c"
      },
      "source": [
        "print(\"Validation Set - AUCCURACY : \",accuracy_score(y_val,model.predict_classes(x_val)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Set - AUCCURACY :  0.4383561643835616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wyNP8BPaXmY"
      },
      "source": [
        "# Now model will predict on the test data.\n",
        "# Best score has got with this model 37.87\n",
        "\n",
        "ans_cnn = model.predict_classes(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKJKG_AEaXsK"
      },
      "source": [
        "# This writes the predicted data into the required kaggle submission file.\n",
        "\n",
        "f = open('cnn1_47val.csv','w')\n",
        "s = \"ID,Labels\\n\"\n",
        "c = 1001\n",
        "for i in ans_cnn:\n",
        "  s = s + c.__str__() + \",\" + i.__str__()+\"\\n\"\n",
        "  c += 1\n",
        "f.write(s)\n",
        "f.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JrRAHqNnTs-"
      },
      "source": [
        "Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYT1qAEIjSKj"
      },
      "source": [
        "#import libraries that are required to perform \n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr6Lan-6m0ps"
      },
      "source": [
        "# Number of iterations has been set as 100. This I have tried with different max iteration values.\n",
        "\n",
        "mlp_gs = MLPClassifier(max_iter=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEKF6WWSm0fj"
      },
      "source": [
        "#Parameter tuning with gridsearchcv\n",
        "\n",
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(10,30,10),(20,)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYxcvVu5m0Qk"
      },
      "source": [
        "# Cross Validation is set as 10.\n",
        "clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, verbose = 50 ,cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ITixcxSm0Fs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5db819d-f5ce-4b3f-fd0c-336086a3b3d9"
      },
      "source": [
        "# Model is being trained with splitted data\n",
        "\n",
        "clf.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    1.6s\n",
            "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.6s\n",
            "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    1.6s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    1.8s\n",
            "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    1.8s\n",
            "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.2s\n",
            "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.2s\n",
            "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    2.4s\n",
            "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    2.4s\n",
            "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    2.7s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    2.7s\n",
            "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:    2.9s\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    2.9s\n",
            "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    3.0s\n",
            "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.0s\n",
            "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:    3.2s\n",
            "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    3.2s\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    3.4s\n",
            "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:    3.4s\n",
            "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:    3.6s\n",
            "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    3.7s\n",
            "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.9s\n",
            "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.9s\n",
            "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:    4.1s\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    4.1s\n",
            "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    4.2s\n",
            "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    4.3s\n",
            "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:    4.5s\n",
            "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    4.6s\n",
            "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    4.7s\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.8s\n",
            "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:    5.0s\n",
            "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:    5.0s\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:    5.2s\n",
            "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    5.3s\n",
            "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:    5.4s\n",
            "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    5.5s\n",
            "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:    5.6s\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.7s\n",
            "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:    5.8s\n",
            "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:    5.8s\n",
            "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    5.9s\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    6.0s\n",
            "[Parallel(n_jobs=-1)]: Done  47 tasks      | elapsed:    6.1s\n",
            "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    6.1s\n",
            "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:    6.3s\n",
            "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    6.3s\n",
            "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed:    6.5s\n",
            "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    6.5s\n",
            "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    6.7s\n",
            "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:    6.7s\n",
            "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed:    6.9s\n",
            "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    6.9s\n",
            "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:    7.0s\n",
            "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:    7.0s\n",
            "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:    7.2s\n",
            "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:    7.2s\n",
            "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:    7.4s\n",
            "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:    7.4s\n",
            "[Parallel(n_jobs=-1)]: Done  63 tasks      | elapsed:    7.6s\n",
            "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    7.6s\n",
            "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:    7.7s\n",
            "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    7.7s\n",
            "[Parallel(n_jobs=-1)]: Done  67 tasks      | elapsed:    7.9s\n",
            "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:    7.9s\n",
            "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:    8.0s\n",
            "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:    8.1s\n",
            "[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed:    8.3s\n",
            "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:    8.3s\n",
            "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed:    8.4s\n",
            "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:    8.5s\n",
            "[Parallel(n_jobs=-1)]: Done  75 tasks      | elapsed:    8.6s\n",
            "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    8.7s\n",
            "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:    8.8s\n",
            "[Parallel(n_jobs=-1)]: Done  78 tasks      | elapsed:    8.9s\n",
            "[Parallel(n_jobs=-1)]: Done  79 tasks      | elapsed:    9.0s\n",
            "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:    9.0s\n",
            "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    9.1s\n",
            "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:    9.3s\n",
            "[Parallel(n_jobs=-1)]: Done  83 tasks      | elapsed:    9.3s\n",
            "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:    9.3s\n",
            "[Parallel(n_jobs=-1)]: Done  85 tasks      | elapsed:    9.4s\n",
            "[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed:    9.4s\n",
            "[Parallel(n_jobs=-1)]: Done  87 tasks      | elapsed:    9.5s\n",
            "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    9.6s\n",
            "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:    9.6s\n",
            "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:    9.7s\n",
            "[Parallel(n_jobs=-1)]: Done  91 tasks      | elapsed:    9.8s\n",
            "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed:    9.9s\n",
            "[Parallel(n_jobs=-1)]: Done  93 tasks      | elapsed:   10.1s\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:   10.1s\n",
            "[Parallel(n_jobs=-1)]: Done  95 tasks      | elapsed:   10.3s\n",
            "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   10.4s\n",
            "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   10.5s\n",
            "[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:   10.6s\n",
            "[Parallel(n_jobs=-1)]: Done  99 tasks      | elapsed:   10.8s\n",
            "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:   10.8s\n",
            "[Parallel(n_jobs=-1)]: Done 101 tasks      | elapsed:   11.0s\n",
            "[Parallel(n_jobs=-1)]: Done 102 tasks      | elapsed:   11.0s\n",
            "[Parallel(n_jobs=-1)]: Done 103 tasks      | elapsed:   11.2s\n",
            "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:   11.3s\n",
            "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:   11.4s\n",
            "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:   11.5s\n",
            "[Parallel(n_jobs=-1)]: Done 107 tasks      | elapsed:   11.7s\n",
            "[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed:   11.7s\n",
            "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:   11.9s\n",
            "[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed:   11.9s\n",
            "[Parallel(n_jobs=-1)]: Done 111 tasks      | elapsed:   12.1s\n",
            "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   12.1s\n",
            "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   12.2s\n",
            "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed:   12.4s\n",
            "[Parallel(n_jobs=-1)]: Done 115 tasks      | elapsed:   12.4s\n",
            "[Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed:   12.6s\n",
            "[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:   12.7s\n",
            "[Parallel(n_jobs=-1)]: Done 118 tasks      | elapsed:   12.7s\n",
            "[Parallel(n_jobs=-1)]: Done 119 tasks      | elapsed:   12.9s\n",
            "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   12.9s\n",
            "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:   13.1s\n",
            "[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed:   13.1s\n",
            "[Parallel(n_jobs=-1)]: Done 123 tasks      | elapsed:   13.3s\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:   13.3s\n",
            "[Parallel(n_jobs=-1)]: Done 125 tasks      | elapsed:   13.4s\n",
            "[Parallel(n_jobs=-1)]: Done 126 tasks      | elapsed:   13.4s\n",
            "[Parallel(n_jobs=-1)]: Done 127 tasks      | elapsed:   13.6s\n",
            "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:   13.6s\n",
            "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:   13.8s\n",
            "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   13.8s\n",
            "[Parallel(n_jobs=-1)]: Done 131 tasks      | elapsed:   13.9s\n",
            "[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed:   13.9s\n",
            "[Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed:   14.1s\n",
            "[Parallel(n_jobs=-1)]: Done 134 tasks      | elapsed:   14.1s\n",
            "[Parallel(n_jobs=-1)]: Done 135 tasks      | elapsed:   14.3s\n",
            "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:   14.3s\n",
            "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:   14.4s\n",
            "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   14.5s\n",
            "[Parallel(n_jobs=-1)]: Done 139 tasks      | elapsed:   14.6s\n",
            "[Parallel(n_jobs=-1)]: Done 140 tasks      | elapsed:   14.7s\n",
            "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed:   14.8s\n",
            "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:   14.8s\n",
            "[Parallel(n_jobs=-1)]: Done 143 tasks      | elapsed:   15.0s\n",
            "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:   15.0s\n",
            "[Parallel(n_jobs=-1)]: Done 145 tasks      | elapsed:   15.2s\n",
            "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   15.2s\n",
            "[Parallel(n_jobs=-1)]: Done 147 tasks      | elapsed:   15.3s\n",
            "[Parallel(n_jobs=-1)]: Done 148 tasks      | elapsed:   15.3s\n",
            "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   15.5s\n",
            "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed:   15.5s\n",
            "[Parallel(n_jobs=-1)]: Done 151 tasks      | elapsed:   15.6s\n",
            "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:   15.7s\n",
            "[Parallel(n_jobs=-1)]: Done 153 tasks      | elapsed:   15.8s\n",
            "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   15.9s\n",
            "[Parallel(n_jobs=-1)]: Done 155 tasks      | elapsed:   16.0s\n",
            "[Parallel(n_jobs=-1)]: Done 156 tasks      | elapsed:   16.0s\n",
            "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:   16.2s\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:   16.2s\n",
            "[Parallel(n_jobs=-1)]: Done 159 tasks      | elapsed:   16.4s\n",
            "[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed:   16.4s\n",
            "[Parallel(n_jobs=-1)]: Done 161 tasks      | elapsed:   16.6s\n",
            "[Parallel(n_jobs=-1)]: Done 162 tasks      | elapsed:   16.6s\n",
            "[Parallel(n_jobs=-1)]: Done 163 tasks      | elapsed:   16.8s\n",
            "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:   16.8s\n",
            "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:   16.8s\n",
            "[Parallel(n_jobs=-1)]: Done 166 tasks      | elapsed:   17.1s\n",
            "[Parallel(n_jobs=-1)]: Done 167 tasks      | elapsed:   17.1s\n",
            "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   17.1s\n",
            "[Parallel(n_jobs=-1)]: Done 169 tasks      | elapsed:   17.1s\n",
            "[Parallel(n_jobs=-1)]: Done 170 tasks      | elapsed:   17.2s\n",
            "[Parallel(n_jobs=-1)]: Done 171 tasks      | elapsed:   17.4s\n",
            "[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed:   17.4s\n",
            "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:   17.6s\n",
            "[Parallel(n_jobs=-1)]: Done 174 tasks      | elapsed:   17.6s\n",
            "[Parallel(n_jobs=-1)]: Done 175 tasks      | elapsed:   17.8s\n",
            "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   17.9s\n",
            "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed:   18.0s\n",
            "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   18.1s\n",
            "[Parallel(n_jobs=-1)]: Done 179 tasks      | elapsed:   18.3s\n",
            "[Parallel(n_jobs=-1)]: Done 180 tasks      | elapsed:   18.3s\n",
            "[Parallel(n_jobs=-1)]: Done 181 tasks      | elapsed:   18.4s\n",
            "[Parallel(n_jobs=-1)]: Done 182 tasks      | elapsed:   18.4s\n",
            "[Parallel(n_jobs=-1)]: Done 183 tasks      | elapsed:   18.6s\n",
            "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   18.6s\n",
            "[Parallel(n_jobs=-1)]: Done 185 tasks      | elapsed:   18.9s\n",
            "[Parallel(n_jobs=-1)]: Done 186 tasks      | elapsed:   18.9s\n",
            "[Parallel(n_jobs=-1)]: Done 187 tasks      | elapsed:   19.1s\n",
            "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:   19.1s\n",
            "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   19.3s\n",
            "[Parallel(n_jobs=-1)]: Done 190 tasks      | elapsed:   19.3s\n",
            "[Parallel(n_jobs=-1)]: Done 191 tasks      | elapsed:   19.5s\n",
            "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   19.5s\n",
            "[Parallel(n_jobs=-1)]: Done 193 tasks      | elapsed:   19.7s\n",
            "[Parallel(n_jobs=-1)]: Done 194 tasks      | elapsed:   19.7s\n",
            "[Parallel(n_jobs=-1)]: Done 195 tasks      | elapsed:   19.9s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   20.0s\n",
            "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed:   20.2s\n",
            "[Parallel(n_jobs=-1)]: Done 198 tasks      | elapsed:   20.2s\n",
            "[Parallel(n_jobs=-1)]: Done 199 tasks      | elapsed:   20.4s\n",
            "[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:   20.4s\n",
            "[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:   20.6s\n",
            "[Parallel(n_jobs=-1)]: Done 202 tasks      | elapsed:   20.6s\n",
            "[Parallel(n_jobs=-1)]: Done 203 tasks      | elapsed:   20.7s\n",
            "[Parallel(n_jobs=-1)]: Done 204 tasks      | elapsed:   20.7s\n",
            "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:   20.9s\n",
            "[Parallel(n_jobs=-1)]: Done 206 tasks      | elapsed:   20.9s\n",
            "[Parallel(n_jobs=-1)]: Done 207 tasks      | elapsed:   21.0s\n",
            "[Parallel(n_jobs=-1)]: Done 208 tasks      | elapsed:   21.1s\n",
            "[Parallel(n_jobs=-1)]: Done 209 tasks      | elapsed:   21.1s\n",
            "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   21.2s\n",
            "[Parallel(n_jobs=-1)]: Done 211 tasks      | elapsed:   21.3s\n",
            "[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed:   21.4s\n",
            "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:   21.5s\n",
            "[Parallel(n_jobs=-1)]: Done 214 tasks      | elapsed:   21.6s\n",
            "[Parallel(n_jobs=-1)]: Done 215 tasks      | elapsed:   21.7s\n",
            "[Parallel(n_jobs=-1)]: Done 216 tasks      | elapsed:   21.8s\n",
            "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:   21.8s\n",
            "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed:   21.9s\n",
            "[Parallel(n_jobs=-1)]: Done 219 tasks      | elapsed:   22.0s\n",
            "[Parallel(n_jobs=-1)]: Done 220 tasks      | elapsed:   22.1s\n",
            "[Parallel(n_jobs=-1)]: Done 221 tasks      | elapsed:   22.2s\n",
            "[Parallel(n_jobs=-1)]: Done 222 tasks      | elapsed:   22.3s\n",
            "[Parallel(n_jobs=-1)]: Done 223 tasks      | elapsed:   22.4s\n",
            "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:   22.4s\n",
            "[Parallel(n_jobs=-1)]: Done 225 tasks      | elapsed:   22.5s\n",
            "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:   22.6s\n",
            "[Parallel(n_jobs=-1)]: Done 227 tasks      | elapsed:   22.7s\n",
            "[Parallel(n_jobs=-1)]: Done 228 tasks      | elapsed:   22.8s\n",
            "[Parallel(n_jobs=-1)]: Done 229 tasks      | elapsed:   22.8s\n",
            "[Parallel(n_jobs=-1)]: Done 230 tasks      | elapsed:   23.0s\n",
            "[Parallel(n_jobs=-1)]: Done 231 tasks      | elapsed:   23.0s\n",
            "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:   23.1s\n",
            "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   23.2s\n",
            "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:   23.3s\n",
            "[Parallel(n_jobs=-1)]: Done 235 tasks      | elapsed:   23.4s\n",
            "[Parallel(n_jobs=-1)]: Done 236 tasks      | elapsed:   23.5s\n",
            "[Parallel(n_jobs=-1)]: Done 237 tasks      | elapsed:   23.6s\n",
            "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed:   23.6s\n",
            "[Parallel(n_jobs=-1)]: Done 239 tasks      | elapsed:   23.8s\n",
            "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed:   23.8s\n",
            "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed:   24.0s\n",
            "[Parallel(n_jobs=-1)]: Done 242 tasks      | elapsed:   24.0s\n",
            "[Parallel(n_jobs=-1)]: Done 243 tasks      | elapsed:   24.0s\n",
            "[Parallel(n_jobs=-1)]: Done 244 tasks      | elapsed:   24.1s\n",
            "[Parallel(n_jobs=-1)]: Done 245 tasks      | elapsed:   24.2s\n",
            "[Parallel(n_jobs=-1)]: Done 246 tasks      | elapsed:   24.3s\n",
            "[Parallel(n_jobs=-1)]: Done 247 tasks      | elapsed:   24.3s\n",
            "[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed:   24.5s\n",
            "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:   24.5s\n",
            "[Parallel(n_jobs=-1)]: Done 250 tasks      | elapsed:   24.7s\n",
            "[Parallel(n_jobs=-1)]: Done 251 tasks      | elapsed:   24.7s\n",
            "[Parallel(n_jobs=-1)]: Done 252 tasks      | elapsed:   24.9s\n",
            "[Parallel(n_jobs=-1)]: Done 253 tasks      | elapsed:   24.9s\n",
            "[Parallel(n_jobs=-1)]: Done 254 tasks      | elapsed:   25.1s\n",
            "[Parallel(n_jobs=-1)]: Done 255 tasks      | elapsed:   25.2s\n",
            "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   25.4s\n",
            "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:   25.4s\n",
            "[Parallel(n_jobs=-1)]: Done 258 tasks      | elapsed:   25.6s\n",
            "[Parallel(n_jobs=-1)]: Done 259 tasks      | elapsed:   25.6s\n",
            "[Parallel(n_jobs=-1)]: Done 260 tasks      | elapsed:   25.8s\n",
            "[Parallel(n_jobs=-1)]: Done 261 tasks      | elapsed:   25.8s\n",
            "[Parallel(n_jobs=-1)]: Done 262 tasks      | elapsed:   26.0s\n",
            "[Parallel(n_jobs=-1)]: Done 263 tasks      | elapsed:   26.0s\n",
            "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:   26.2s\n",
            "[Parallel(n_jobs=-1)]: Done 265 tasks      | elapsed:   26.2s\n",
            "[Parallel(n_jobs=-1)]: Done 266 tasks      | elapsed:   26.3s\n",
            "[Parallel(n_jobs=-1)]: Done 267 tasks      | elapsed:   26.4s\n",
            "[Parallel(n_jobs=-1)]: Done 268 tasks      | elapsed:   26.5s\n",
            "[Parallel(n_jobs=-1)]: Done 269 tasks      | elapsed:   26.5s\n",
            "[Parallel(n_jobs=-1)]: Done 270 tasks      | elapsed:   26.7s\n",
            "[Parallel(n_jobs=-1)]: Done 271 tasks      | elapsed:   26.8s\n",
            "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:   27.0s\n",
            "[Parallel(n_jobs=-1)]: Done 273 tasks      | elapsed:   27.0s\n",
            "[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:   27.2s\n",
            "[Parallel(n_jobs=-1)]: Done 275 tasks      | elapsed:   27.2s\n",
            "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:   27.4s\n",
            "[Parallel(n_jobs=-1)]: Done 277 tasks      | elapsed:   27.4s\n",
            "[Parallel(n_jobs=-1)]: Done 278 tasks      | elapsed:   27.6s\n",
            "[Parallel(n_jobs=-1)]: Done 279 tasks      | elapsed:   27.7s\n",
            "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   27.8s\n",
            "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   27.9s\n",
            "[Parallel(n_jobs=-1)]: Done 282 tasks      | elapsed:   27.9s\n",
            "[Parallel(n_jobs=-1)]: Done 283 tasks      | elapsed:   28.0s\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:   28.1s\n",
            "[Parallel(n_jobs=-1)]: Done 285 tasks      | elapsed:   28.2s\n",
            "[Parallel(n_jobs=-1)]: Done 286 tasks      | elapsed:   28.2s\n",
            "[Parallel(n_jobs=-1)]: Done 287 tasks      | elapsed:   28.2s\n",
            "[Parallel(n_jobs=-1)]: Done 288 tasks      | elapsed:   28.3s\n",
            "[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed:   28.4s\n",
            "[Parallel(n_jobs=-1)]: Done 290 tasks      | elapsed:   28.5s\n",
            "[Parallel(n_jobs=-1)]: Done 291 tasks      | elapsed:   28.6s\n",
            "[Parallel(n_jobs=-1)]: Done 292 tasks      | elapsed:   28.6s\n",
            "[Parallel(n_jobs=-1)]: Done 293 tasks      | elapsed:   28.7s\n",
            "[Parallel(n_jobs=-1)]: Done 294 tasks      | elapsed:   28.8s\n",
            "[Parallel(n_jobs=-1)]: Done 295 tasks      | elapsed:   28.9s\n",
            "[Parallel(n_jobs=-1)]: Done 296 tasks      | elapsed:   28.9s\n",
            "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   29.1s\n",
            "[Parallel(n_jobs=-1)]: Done 298 tasks      | elapsed:   29.1s\n",
            "[Parallel(n_jobs=-1)]: Done 299 tasks      | elapsed:   29.3s\n",
            "[Parallel(n_jobs=-1)]: Done 300 tasks      | elapsed:   29.3s\n",
            "[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:   29.4s\n",
            "[Parallel(n_jobs=-1)]: Done 302 tasks      | elapsed:   29.5s\n",
            "[Parallel(n_jobs=-1)]: Done 303 tasks      | elapsed:   29.6s\n",
            "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:   29.6s\n",
            "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:   29.8s\n",
            "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   29.8s\n",
            "[Parallel(n_jobs=-1)]: Done 307 tasks      | elapsed:   29.9s\n",
            "[Parallel(n_jobs=-1)]: Done 308 tasks      | elapsed:   30.0s\n",
            "[Parallel(n_jobs=-1)]: Done 309 tasks      | elapsed:   30.1s\n",
            "[Parallel(n_jobs=-1)]: Done 310 tasks      | elapsed:   30.1s\n",
            "[Parallel(n_jobs=-1)]: Done 311 tasks      | elapsed:   30.3s\n",
            "[Parallel(n_jobs=-1)]: Done 312 tasks      | elapsed:   30.3s\n",
            "[Parallel(n_jobs=-1)]: Done 313 tasks      | elapsed:   30.5s\n",
            "[Parallel(n_jobs=-1)]: Done 314 tasks      | elapsed:   30.5s\n",
            "[Parallel(n_jobs=-1)]: Done 315 tasks      | elapsed:   30.7s\n",
            "[Parallel(n_jobs=-1)]: Done 316 tasks      | elapsed:   30.7s\n",
            "[Parallel(n_jobs=-1)]: Done 317 tasks      | elapsed:   30.8s\n",
            "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:   31.0s finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score=nan,\n",
              "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
              "                                     batch_size='auto', beta_1=0.9,\n",
              "                                     beta_2=0.999, early_stopping=False,\n",
              "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
              "                                     learning_rate='constant',\n",
              "                                     learning_rate_init=0.001, max_fun=15000,\n",
              "                                     max_iter=100, momentum=0.9,\n",
              "                                     n_iter_no_change=10,\n",
              "                                     nesterovs_momentum=True, power_t=0.5,\n",
              "                                     random_stat...\n",
              "                                     solver='adam', tol=0.0001,\n",
              "                                     validation_fraction=0.1, verbose=False,\n",
              "                                     warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'activation': ['tanh', 'relu'],\n",
              "                         'alpha': [0.0001, 0.05],\n",
              "                         'hidden_layer_sizes': [(10, 30, 10), (20,)],\n",
              "                         'learning_rate': ['constant', 'adaptive'],\n",
              "                         'solver': ['sgd', 'adam']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKxfoQUsmz43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b8017a-6fb5-4460-94bf-8b6ae7b45f0f"
      },
      "source": [
        "# Out of the given parameters the best parameters are printed.\n",
        "\n",
        "print('Best parameters found:\\n', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best parameters found:\n",
            " {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'constant', 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToAmrCqdmzuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdd42827-cf6a-44b2-9613-f7dfff179d62"
      },
      "source": [
        "## Various Means and stds scores are mentioned her.\n",
        "\n",
        "means = clf.cv_results_['mean_test_score']\n",
        "stds = clf.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.532 (+/-0.125) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.545 (+/-0.244) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.550 (+/-0.139) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.551 (+/-0.250) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.531 (+/-0.209) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.533 (+/-0.246) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.557 (+/-0.209) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.581 (+/-0.246) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.586 (+/-0.106) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.586 (+/-0.232) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.562 (+/-0.107) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.563 (+/-0.271) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.533 (+/-0.187) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.545 (+/-0.290) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.538 (+/-0.217) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.557 (+/-0.256) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.539 (+/-0.154) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.532 (+/-0.221) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.509 (+/-0.163) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.574 (+/-0.255) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.545 (+/-0.144) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.550 (+/-0.204) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.567 (+/-0.174) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.568 (+/-0.245) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.586 (+/-0.167) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.503 (+/-0.264) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.533 (+/-0.130) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.574 (+/-0.165) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.543 (+/-0.256) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.544 (+/-0.240) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.561 (+/-0.160) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.551 (+/-0.256) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (20,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04ci4fRQmyeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860b8aa7-6f1a-4160-c4f9-d5b8b68fdc58"
      },
      "source": [
        "print(' Validation score is : ',accuracy_score(y_val,clf.predict(x_val)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Validation score is :  0.5205479452054794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBG4R9f455U-"
      },
      "source": [
        "# Prediction is done on test data \n",
        "\n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlJGc_bd55bC",
        "outputId": "57d192d3-d02d-4769-dda4-3bcfe7a1e4d4"
      },
      "source": [
        "print('The prediction on test data is ',y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The prediction on test data is  [0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E-6yAch55hS"
      },
      "source": [
        "# This writes the predicted data into the required kaggle submission file.\n",
        "\n",
        "f = open('mlp_53point42val.csv','w')\n",
        "s = \"ID,Labels\\n\"\n",
        "c = 1001\n",
        "for i in y_pred:\n",
        "  s = s + c.__str__() + \",\" + i.__str__()+\"\\n\"\n",
        "  c += 1\n",
        "f.write(s)\n",
        "f.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3DJ5-KV4lti"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}